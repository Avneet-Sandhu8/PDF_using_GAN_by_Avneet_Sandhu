# ğŸ“Š PDF Learning using GAN - NOâ‚‚ Concentration Distribution

**ğŸ‘¤ Name:** Avneet Sandhu  
**ğŸ“ Roll Number:** 102303289  

---

## Objective

To learn the unknown probability density function (PDF) of a transformed NOâ‚‚ concentration variable using a Generative Adversarial Network (GAN).

---

## ğŸ“Š Dataset

**Feature used:**
- **NOâ‚‚ concentration** (`no2` column)
- **Source:** Air quality monitoring dataset
- **Type:** Continuous numerical data representing Nitrogen Dioxide levels

---

## ğŸ”„ Transformation

The original variable `x` is transformed into:

```
z = x + aáµ£ Â· sin(báµ£ Â· x)
```

**Where:**
- `aáµ£ = 0.5 Ã— (r mod 7)`
- `báµ£ = 0.3 Ã— (r mod 5 + 1)`

**For roll number: 102303289**
- **aáµ£ = 2.0**
- **báµ£ = 1.5**

**Final Transformation:**
```
z = x + 2.0 Â· sin(1.5 Â· x)
```

This transformation introduces non-linearity through the sine function, creating a complex distribution that the GAN must learn.

---

## Methodology

### 1. **Data Preprocessing**
   - Load NOâ‚‚ concentration data from the dataset
   - Handle missing values and outliers
   - Extract the `no2` column for analysis

### 2. **Transformation of NOâ‚‚ Values**
   - Apply the non-linear transformation: `z = x + 2.0 Â· sin(1.5 Â· x)`
   - Create a more complex distribution from the original data

### 3. **Standardization of Data**
   - Normalize the transformed data using Z-score normalization
   - Ensure data is in a suitable range for GAN training

### 4. **GAN Training**
   - **Generator Network:** 
     - Generates synthetic samples from random noise
     - Architecture: Fully connected layers with ReLU/LeakyReLU activations
   - **Discriminator Network:**
     - Distinguishes between real and fake samples
     - Architecture: Fully connected layers with LeakyReLU and Dropout
   - **Training Process:**
     - Adversarial training with alternating updates
     - Binary Cross-Entropy loss function
     - Adam optimizer for both networks

### 5. **Generation of Fake Samples**
   - Use the trained generator to produce synthetic samples
   - Generate large dataset (e.g., 10,000 samples) for analysis

### 6. **PDF Estimation using Kernel Density Estimation (KDE)**
   - Apply KDE to both real and generated samples
   - Use Gaussian kernel for smooth probability density estimation
   - Automatic bandwidth selection

### 7. **Comparison of Real vs GAN Learned Distribution**
   - Visual comparison through plots
   - Statistical comparison using metrics
   - Evaluation of distribution learning quality

---

## Results

### ğŸ”¹ PDF Comparison
![PDF Comparison](results/pdf_comparison.png)

**Description:** This plot compares the probability density functions of the real transformed NOâ‚‚ data (blue) against the GAN-learned distribution (orange). The close alignment demonstrates the GAN's ability to accurately capture the underlying probability distribution.

---

### Histogram Comparison
![Histogram Comparison](results/histogram_comparison.png)

**Description:** Histogram overlay showing the frequency distribution of real samples versus GAN-generated samples. The overlapping bars indicate successful distribution matching across different value ranges.

---

### Zoomed PDF Comparison
![Zoomed PDF](results/zoomed_pdf_comparison.png)

**Description:** A detailed, zoomed-in view of the PDF comparison, highlighting the precision of the GAN-learned distribution in capturing subtle features, peaks, and tail behaviors of the real data distribution.

---

## Repository Structure

```
PDF_using_GAN_by_Avneet_Sandhu/
â”‚
â”œâ”€â”€ README.md                          
â”‚
â”œâ”€â”€ results/                          
â”‚   â”œâ”€â”€ histogram_comparison.png      
â”‚   â”œâ”€â”€ pdf_comparison.png            
â”‚   â””â”€â”€ zoomed_pdf_comparison.png    
â”‚
â”œâ”€â”€ Dataset.zip                       
â”‚   â””â”€â”€ data.csv                           
â””â”€â”€ PDF_using_GAN.ipynb                         
```
---


<div align="center">
  
### ğŸ¯ Made with â¤ï¸ for Machine Learning and Deep Learning

**Happy Learning! ğŸš€**

</div>
